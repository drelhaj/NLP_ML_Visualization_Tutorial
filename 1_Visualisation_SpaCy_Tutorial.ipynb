{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising Text with SpaCy\n",
    "#author: Dr Mahmoud El-Haj (with help from the Internet) as part of the \"Data Visualisation Workshop for Critical Computational Discourse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use SpaCy, a python package with libraries to analyse and anotate text.\n",
    "#http://spacy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing spaCy\n",
    "#https://spacy.io/usage\n",
    "#!pip install -U spacy\n",
    "#!pip install -U spacy-lookups-data\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy Tokenizer Construction\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "import spacy    \n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Today is March 19th 2020 and Mahmoud is showing us how to visualising text at Lancaster University.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(sentence)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what about stop-words?\n",
    "#SpaCy's English language stop words (for other languages see: https://spacy.io/usage/models)\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get tokens ignoring stop-words and punctuations\n",
    "tokens_no_stopwords = [token.text for token in tokens if token.is_stop != True and token.is_punct != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*tokens, len(tokens))\n",
    "print(*tokens_no_stopwords, len(tokens_no_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what if we want to add/remove to/from the default stop-words list?\n",
    "#assume the word 'text' is very frequent in our corpus to an extent that it becomes a stop-word\n",
    "#to add 'text' to the stop words list:\n",
    "nlp.Defaults.stop_words.add(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the list, notice 'text' is now an entry\n",
    "#to remove a word from the list use: nlp.Defaults.stop_words.remove(\"word_to_be_removed\")\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokens = tokenizer(sentence)\n",
    "#loop through the tokens and only consider non-stop-words and non-punctuations.\n",
    "tokens_no_stopwords = [token.text for token in tokens if token.is_stop != True and token.is_punct != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*tokens_no_stopwords, len(tokens_no_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linguistic annotations (Part of speech tags and dependencies) \n",
    "#This will return a Language object containing all components and data needed to process text\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualise the annotated sentence above\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentence)\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can we make it look a bit cooler? (for more options https://spacy.io/api/top-level#displacy_options)\n",
    "options = {\"compact\": True, \"bg\": \"#09a3d5\",\n",
    "           \"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
    "displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what about named entities?\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
