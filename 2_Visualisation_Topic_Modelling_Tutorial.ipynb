{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part 2 of the 5 parts tutorial shows you how to topic modelling! \n",
    "#Our data-set is a list of talks and abstracts from the CCC conference https://gitlab.com/maxigas/cccongresstalks/\n",
    "#The data is available as .csv file format, we'll play with the data to show what topics are being focused on \n",
    "#    using Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "#this is to avoid showing warning, comment out otherwise.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mglearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the 2016.csv CCC talks, which is stored as a CSV file in the csvs directory.\n",
    "#notice the delimiter is not a comma, check your files first.\n",
    "File2016_df = pd.read_csv(\"csvs/2016.csv\", delimiter='|', header=0)\n",
    "print('Number of titles: {:,}\\n'.format(File2016_df.shape[0]))\n",
    "#print a sample of 5 rows.\n",
    "File2016_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading titles (title is a column in the csv file as shown in the sample above)\n",
    "\n",
    "#loop through the titles and store them in a dictionary.\n",
    "title_dict = {}\n",
    "\n",
    "# loop through titles\n",
    "for i in range(len(File2016_df)):\n",
    "    if File2016_df[\"title\"][i] in title_dict.keys():\n",
    "        title_dict[File2016_df[\"title\"][i]] += 1\n",
    "    else:\n",
    "        title_dict.setdefault(File2016_df[\"title\"][i], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we do the same thing for the abstract column\n",
    "\n",
    "abstract_dict = {}\n",
    "\n",
    "# classify that the article has recommends\n",
    "for i in range(len(File2016_df)):\n",
    "    if File2016_df[\"abstract\"][i] in abstract_dict.keys():\n",
    "        abstract_dict[File2016_df[\"abstract\"][i]] += 1\n",
    "    else:\n",
    "        abstract_dict.setdefault(File2016_df[\"abstract\"][i], 1)\n",
    "\n",
    "abstract_dict = [x for x in abstract_dict if str(x) != 'nan']#some talks have no abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy tokenizer\n",
    "#a spacy method to extract clean tokens from text\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    all_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            all_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            all_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            all_tokens.append(token.lower_)\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK’s Wordnet used to find the meanings of words, synonyms, antonyms, and more. \n",
    "#I import English and German stop words lists since the talks contain German text, especially in the old days.\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "de_stop = set(nltk.corpus.stopwords.words('german'))\n",
    "\n",
    "#preprocess text by removing stop words and keeping words with more than 4 letters (my choice you can change it)\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop or token in de_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I convert the dictionary into a list with clean pre-processed text\n",
    "title_text_data = []\n",
    "print(type(title_text_data))\n",
    "counter = 0\n",
    "for title in title_dict:\n",
    "    counter = counter + 1\n",
    "    title_tokens = preprocess_text(title.replace('attackz', 'attacks').replace('hackz', 'hack').replace('securityz', 'security'))#I do some replacing for German-style words!\n",
    "    if (counter < 11):#this is just to print the first 10 lines\n",
    "        print(title_tokens)\n",
    "    title_text_data.append(title_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The same is done to the abstracts dictionary\n",
    "abstract_text_data = []\n",
    "counter2 = 0\n",
    "for abstract in abstract_dict:\n",
    "    counter2 = counter2 + 1\n",
    "    abstract_tokens = preprocess_text(abstract.replace('attackz', 'attacks').replace('hackz', 'hack').replace('securityz', 'security'))\n",
    "    if (counter2 < 11):\n",
    "        print(abstract_tokens)\n",
    "    abstract_text_data.append(abstract_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim for titles\n",
    "#what is a Gensim? The next important object you need to familiarize with in order to work in gensim is the Corpus (a Bag of Words). That is, it is a corpus object that contains the word id and its frequency in each document. You can think of it as gensim's equivalent of a Document-Term matrix\n",
    "#The main function is doc2bow , which converts a collection of words to its bag-of-words representation: a list of (word_id, word_frequency) 2-tuples\n",
    "from gensim import corpora\n",
    "title_dictionary = corpora.Dictionary(title_text_data)\n",
    "title_corpus = [title_dictionary.doc2bow(text) for text in title_text_data]\n",
    "\n",
    "import pickle\n",
    "#Pickle is needed to serialize gensim into a stream of bytes to be stored as a file.\n",
    "#gensim and pkl files are stored in the 'models' directory.\n",
    "pickle.dump(title_corpus, open('models/title_corpus.pkl', 'wb'))\n",
    "title_dictionary.save('models/title_dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim for abstracts\n",
    "from gensim import corpora\n",
    "abstract_dictionary = corpora.Dictionary(abstract_text_data)\n",
    "abstract_corpus = [abstract_dictionary.doc2bow(text) for text in abstract_text_data]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(abstract_corpus, open('models/abstract_corpus.pkl', 'wb'))\n",
    "abstract_dictionary.save('models/abstract_dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title LDA topics (I went with 5 topics and 10 passes, you can update that to show more topics or have several passes)\n",
    "# probability distribution\n",
    "# Discover abstract “topics” that occur in a collection of documents (why some parts of the data are similar)\n",
    "# What are the topics they are talking about in the conference talks\n",
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(title_corpus, num_topics = NUM_TOPICS, id2word=title_dictionary, passes=10)\n",
    "ldamodel.save('models/title_model.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try a new sentence on networking security, \n",
    "#LDA will show that topic 3 has the highest probability assigned, and topic 4 has the second highest probability assigned. \n",
    "#The printed probabilities should add up to 1.\n",
    "new_doc = 'Our new technique offers a full spectrum of data privacy and security services and technology'\n",
    "new_doc = preprocess_text(new_doc)\n",
    "new_doc_bow = title_dictionary.doc2bow(new_doc)\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display and plot title topics\n",
    "#Saliency: how important regardless of the term frequency (i.e. some terms are frequent but not informative)\n",
    "#Paper and more details: http://vis.stanford.edu/files/2012-Termite-AVI.pdf\n",
    "#Relevance: how relevant is a term to a certain topic\n",
    "#Paper and more details: https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "#The size of the bubble measures the importance of the topics, relative to the data.\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load('models/title_dictionary.gensim')\n",
    "corpus = pickle.load(open('models/title_corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('models/title_model.gensim')\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, title_corpus, title_dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display, template_type='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
