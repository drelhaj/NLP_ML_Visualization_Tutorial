{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The First International Workshop on Arabic Big Data & AI (IWABigDAI) May 11 and May 12 2022\n",
    "#https://sites.google.com/view/arabicbigdata/home\n",
    "\n",
    "#Tutorial 1: Download Tweets using Tweepy (Twitter API)\n",
    "#author: Dr Mahmoud El-Haj (with help from the Internet)\n",
    "#GitHub repository: https://github.com/drelhaj/NLP_ML_Visualization_Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "import tweepy\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAOmWVwEAAAAAJlJlIkE%2BiAJ3w0AJjGwgaM4ff1g%3Drvd7wmpYm0vnGC8AP3CCNWQw8UTUcPrHgNlPY7PGS2WkppmXrQ')#you need Twitter Academic Account https://developer.twitter.com/en/products/twitter-api/academic-research\n",
    "\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "import datetime        \n",
    "query = \"Lancaster University\"\n",
    "country = \"place_country:GB\"# add country code. QA: Qatar, GB: UK & NI ...etc\n",
    "queryCombined = query + \" \" + country\n",
    "print(queryCombined)\n",
    "start_date = date(2022, 1, 28)\n",
    "end_date = date(2023, 3, 13)\n",
    "printHeaders = 1\n",
    "\n",
    "for single_date in daterange(start_date, end_date):\n",
    "#try:\n",
    "    start_date = single_date.strftime(\"%Y-%m-%dT00:00:00Z\")\n",
    "    dtObj = datetime.datetime.strptime(start_date, \"%Y-%m-%dT00:00:00Z\")\n",
    "    end_date = dtObj + timedelta(days=1)\n",
    "    start_date_str = str(start_date).replace(\" 00\", \"T00\").replace(\"00 \", \"00Z\")\n",
    "    end_date_str = str(end_date).replace(\" 00\", \"T00\")+\"Z\"\n",
    "    print(start_date_str)\n",
    "    print(end_date_str)\n",
    "    downloadTweets(start_date_str,end_date_str,queryCombined,\"csvs/2023-lancs.csv\",client,printHeaders)\n",
    "    printHeaders = 0\n",
    "#except:\n",
    "#    pass\n",
    "\n",
    "    #sleep for random seconds\n",
    "    sleep(randint(2,5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadTweets(start_time, end_time, query, csv_file,client,printHeaders):\n",
    "    import tweepy\n",
    "\n",
    "    import tweepy\n",
    "    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "    import csv\n",
    "    from urlextract import URLExtract\n",
    "\n",
    "    extractor = URLExtract()\n",
    "\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if printHeaders>0:\n",
    "            writer.writerow([\"tweet_id\",\"tweet_text\",\"dateTime\",\"author_id\",\"retweet_count\",\"like_count\",\"quote_count\",\"reply_count\",\"UserLocation\",\"Tweet_Place\",\"URLs\", \"mentions\", \"hashtags\", \"sentiment\"])\n",
    "            file.flush()\n",
    "\n",
    "        #I'm getting the geo location of the tweet as well as the location of the user and setting the number of tweets returned to 10 (minimum) - Max is 100\n",
    "        tweets = client.search_all_tweets(query=query, tweet_fields=['context_annotations','created_at', 'public_metrics'],\n",
    "                                          place_fields=['place_type', 'geo'], user_fields=['location'], expansions='author_id,geo.place_id',\n",
    "                                          start_time=start_time,\n",
    "                                          end_time=end_time, max_results=100)\n",
    "        \n",
    "        if tweets.data is None:\n",
    "            print(\"\")\n",
    "        else:\n",
    "            print(len(tweets.data))\n",
    "\n",
    "            #.flatten(limit=1000)\n",
    "            # Get list of places and users\n",
    "            places = {p[\"id\"]: p for p in tweets.includes['places']}\n",
    "            users = {u[\"id\"]: u for u in tweets.includes['users']}\n",
    "\n",
    "            #loop through the tweets to get the tweet ID, Text, Author ID, User Location, Tweet Location + public_metrics\n",
    "            for tweet in tweets.data:\n",
    "#                 print(tweet.id)\n",
    "#                 print(tweet.text)\n",
    "#                 print(tweet.created_at)\n",
    "#                 print(tweet.author_id)\n",
    "#                 print(tweet.public_metrics['retweet_count'])\n",
    "#                 print(tweet.public_metrics['like_count'])\n",
    "#                 print(tweet.public_metrics['quote_count'])\n",
    "#                 print(tweet.public_metrics['reply_count'])\n",
    "                userLocation = \"\"\n",
    "                placeFullName = \"\"\n",
    "                if users[tweet.author_id]:\n",
    "                    user = users[tweet.author_id]\n",
    "                    userLocation = user.location\n",
    "                if places[tweet.geo['place_id']]:\n",
    "                    place = places[tweet.geo['place_id']]\n",
    "                    placeFullName = place.full_name\n",
    "    #                 print(\"================\")\n",
    "                cleaned_tweet =clean_tweet(tweet.text)\n",
    "                cleaned_tweet =cleaned_tweet.replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "                mentions = re.findall(\"@([a-zA-Z0-9_]{1,50})\", tweet.text)\n",
    "                hashtags = re.findall(\"#([a-zA-Z0-9_]{1,50})\", tweet.text)\n",
    "                urls = extractor.find_urls(tweet.text)\n",
    "                writer.writerow([tweet.id,cleaned_tweet,tweet.created_at,tweet.author_id,tweet.public_metrics['retweet_count'],tweet.public_metrics['like_count'],tweet.public_metrics['quote_count'],tweet.public_metrics['reply_count'],userLocation,placeFullName,urls,mentions,hashtags,get_tweet_sentiment(cleaned_tweet)])\n",
    "                file.flush()\n",
    "\n",
    "    file.close()          \n",
    "\n",
    "\n",
    "    # # Replace the limit=1000 with the maximum number of Tweets you want\n",
    "    # for tweet in tweepy.Paginator(client.search_recent_tweets, query=query,\n",
    "    #                               tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=1000):\n",
    "    #     print(tweet.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # importing regex\n",
    "import string\n",
    " \n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Remove unncessary things from the tweet \n",
    "    like mentions, hashtags, URL links, punctuations\n",
    "    '''\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    # remove mentions\n",
    "    tweet = re.sub(r'@[A-Za-z0-9]+', '', tweet)  \n",
    "\n",
    "    # remove punctuations like quote, exclamation sign, etc.\n",
    "    # we replace them with a space\n",
    "    tweet = re.sub(r'['+string.punctuation+']+', ' ', tweet)\n",
    "\n",
    "    return tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_tweet_sentiment(tweet):\n",
    "\n",
    "  # create TextBlob object of the passed tweet text\n",
    "    blob = TextBlob(clean_tweet(tweet))\n",
    " \n",
    "  # get sentiment\n",
    "    if blob.sentiment.polarity > 0:\n",
    "        sentiment = 'positive'\n",
    "    elif blob.sentiment.polarity < 0:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    " \n",
    "    return sentiment\n",
    "# testing tweet sentiment\n",
    "sample_tweet = \"wwWhy don’t US is doing completely lockdown, @obama we are sitting home @elhaj doing #sacrifice for #ourself & community . while many of others enjoying freedom & doing shopping’s & making other people’s infected#Coronavirus\"\n",
    "cleaned_tweet =clean_tweet(sample_tweet)\n",
    "mentions = re.findall(\"@([a-zA-Z0-9_]{1,50})\", sample_tweet)\n",
    "hashtags = re.findall(\"#([a-zA-Z0-9_]{1,50})\", sample_tweet)\n",
    "print(cleaned_tweet)\n",
    "print(get_tweet_sentiment(sample_tweet)) # Output: negative\n",
    "print(mentions)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlextract import URLExtract\n",
    "\n",
    "extractor = URLExtract()\n",
    "urls = extractor.find_urls(\"This is a link http://www.google.com hey man how it s fgoid www.yahoo.com, lasdkfj asdf asldkfj htts://www.ommak.com lasdkfljasdkf http://hello.org\")\n",
    "print(urls) # prints: ['stackoverflow.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myString = \"This is a link http://www.google.com hey man how it s fgoid www.yahoo.com, lasdkfj asdf asldkfj htts://www.ommak.com lasdkfljasdkf http://hello.org\"\n",
    "print(re.findall(\"(?P<url>https?://[^\\s]+)\", myString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
