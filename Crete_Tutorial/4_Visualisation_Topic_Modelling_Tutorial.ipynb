{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The First International Workshop on Arabic Big Data & AI (IWABigDAI) May 11 and May 12 2022\n",
    "#https://sites.google.com/view/arabicbigdata/home\n",
    "\n",
    "#Tutorial 4: Visualising with Topic Modelling using Latent Dirichlet Allocation (LDA)\n",
    "#author: Dr Mahmoud El-Haj (with help from the Internet)\n",
    "#GitHub repository: https://github.com/drelhaj/NLP_ML_Visualization_Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#The line above is used to catch warnings and errors (careful this will hide all cell's output)\n",
    "\n",
    "#This part 4 of the 4 parts tutorial shows you how to topic modelling! \n",
    "#Our data-set is a list of talks and abstracts from the CCC conference https://gitlab.com/maxigas/cccongresstalks/\n",
    "#The data is available as .csv file format, we'll play with the data to show what topics are being focused on \n",
    "#    using Latent Dirichlet Allocation (LDA).\n",
    "# In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. \n",
    "\n",
    "#this is to avoid showing warning, comment out otherwise.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np # adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "import pandas as pd # needed for data processing and reading CSV files I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt #plotting library\n",
    "import seaborn as sns#data visualization library\n",
    "#import mglearn #Helper functions for the book 'Introduction to machine learning with Python'\n",
    "from sklearn.feature_extraction.text import CountVectorizer #CountVectorizer used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the 2016.csv CCC talks, which is stored as a CSV file in the csvs directory.\n",
    "#notice the delimiter is not a comma, check your files first.\n",
    "#Our data-set is a list of talks and abstracts from the CCC conference https://gitlab.com/maxigas/cccongresstalks/\n",
    "\n",
    "File2022_df = pd.read_csv(\"csvs/2023-Crete.csv\", delimiter=',', header=0, encoding='utf8')\n",
    "print('Number of titles: {:,}\\n'.format(File2022_df.shape[0]))\n",
    "#print a sample of 3 random rows.\n",
    "File2022_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading tweets (tweet_text is a column in the csv file as shown in the sample above)\n",
    "\n",
    "#loop through the abstracts and store them in a dictionary.\n",
    "\n",
    "tweets_dict = {}\n",
    "\n",
    "# classify that the article has recommends\n",
    "for i in range(len(File2022_df)):\n",
    "    if File2022_df[\"TWEETTEXT\"][i] in tweets_dict.keys():\n",
    "        tweets_dict[File2022_df[\"TWEETTEXT\"][i]] += 1\n",
    "    else:\n",
    "        tweets_dict.setdefault(File2022_df[\"TWEETTEXT\"][i], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to show you the dictionary items, you don't need to print it\n",
    "for i in tweets_dict:\n",
    "    print(i, ',', tweets_dict[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy tokenizer\n",
    "#a spacy method to extract clean tokens from text (noticed I added some extra delimiters to the tokenizer)\n",
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    all_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:#checks if a token resembels a URL\n",
    "            all_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):#needed when mentions are used to anonymise users\n",
    "            all_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            all_tokens.append(token.lower_)\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#NLTK’s Wordnet used to find the meanings of words, synonyms, antonyms, and more. \n",
    "#I import English and German stop words lists since the talks contain German text, especially in the earlier conferences.\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('wordnet')#you may turn this one off if you've already downloaded the wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "nltk.download('stopwords')#download the stopword lists from NLTK. Can be turned off if already downloaded\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#preprocess text by removing stop words and keeping words with more than 4 letters (my choice you can change it)\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = re.sub('[^A-Za-z ]+', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) >= 3]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is an example on how the preprocessing works. \n",
    "sentenceTest = \"Attending a seminar today to understand work completed on exploring research priority \"\n",
    "print(preprocess_text(sentenceTest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I convert the dictionary into a list with clean pre-processed text\n",
    "tweet_text_data = []\n",
    "print(type(tweet_text_data))\n",
    "counter = 0\n",
    "for entry in tweets_dict:\n",
    "    counter = counter + 1\n",
    "    tweet_tokens = preprocess_text(entry)\n",
    "    if counter < 11:\n",
    "        print(tweet_tokens)\n",
    "    tweet_text_data.append(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim for titles\n",
    "#what is a Gensim? The next important object you need to familiarize yourself with in order to work in gensim is the Corpus (a Bag of Words). \n",
    "#That is, it is a corpus object that contains the word id and its frequency in each document. \n",
    "#You can think of it as gensim's equivalent of a Document-Term matrix\n",
    "#The main function is doc2bow , which converts a collection of words to its bag-of-words representation: a list of (word_id, word_frequency) 2-tuples\n",
    "from gensim import corpora\n",
    "tweets_dictionary = corpora.Dictionary(tweet_text_data)\n",
    "tweets_corpus = [tweets_dictionary.doc2bow(text) for text in tweet_text_data]\n",
    "\n",
    "import pickle\n",
    "#Pickle is needed to serialize gensim into a stream of bytes to be stored as a file. Serialization refers to the process of converting an object in memory to a byte stream that can be stored on disk or sent over a network.\n",
    "#gensim and pkl files are stored in the 'models' directory. \n",
    "#we save those models to load them later when we plot the LDA model\n",
    "pickle.dump(tweets_corpus, open('models/tweets_corpus.pkl', 'wb'))\n",
    "tweets_dictionary.save('models/tweets_dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title LDA topics (I went with 5 topics and 10 passes, you can update that to show more topics or have several passes --> Number of passes through the corpus during training.)\n",
    "# probability distribution (Jensen–Shannon divergence, mesures similarity between two probability distributions https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)\n",
    "# Discover abstract “topics” that occur in a collection of documents (why some parts of the data are similar)\n",
    "# What are the topics they are talking about in the conference talks.\n",
    "\n",
    "#to run this on the abstracts you need to replace the tweets_corpus and the title_dictionary with the abstract ones from the preivous cell\n",
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(tweets_corpus, num_topics = NUM_TOPICS, id2word=tweets_dictionary, passes=10)\n",
    "ldamodel.save('models/tweets_model.gensim')\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try a new sentence on networking security, \n",
    "#LDA will show that topic 3 has the highest probability assigned, and topic 4 has the second highest probability assigned. \n",
    "#The printed probabilities should add up to 1.\n",
    "new_doc = 'I look forward to my graduation day at Lancaster University'\n",
    "new_doc = preprocess_text(new_doc)\n",
    "new_doc_bow = tweets_dictionary.doc2bow(new_doc)\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#display and plot tweets topics\n",
    "#to understand the model: http://bl.ocks.org/AlessandraSozzi/raw/ce1ace56e4aed6f2d614ae2243aab5a5/\n",
    "#and https://www.youtube.com/watch?v=IksL96ls4o0&feature=emb_title&ab_channel=statgraphics\n",
    "#Saliency: how important regardless of the term frequency (i.e. some terms are frequent but not informative)\n",
    "#Paper and more details: http://vis.stanford.edu/files/2012-Termite-AVI.pdf\n",
    "#Relevance: how relevant is a term to a certain topic\n",
    "#Paper and more details: https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "#The size of the bubble measures the importance of the topics, relative to the data.\n",
    "#more weight on lambda shows terms that are relevant to the topic. Less weight shows terms that are most frequent in that topic (beware of stop-words)\n",
    "\n",
    "#may struggle with numpy >=1.20.x\n",
    "dictionary = gensim.corpora.Dictionary.load('models/tweets_dictionary.gensim')\n",
    "corpus = pickle.load(open('models/tweets_corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('models/tweets_model.gensim')\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()#this to make it run with Jupyter notebook, not needed if using python IDE\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.save_html(lda_display, './plots/tweets_lda.html')#save the LDA visualisation to the models directory for you to use later.\n",
    "pyLDAvis.display(lda_display, template_type='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import gensim\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import arabic_reshaper # this was missing in your code\n",
    "from bidi.algorithm import get_display\n",
    "#lda = gensim.models.ldamodel.LdaModel.load('models/title_model.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeText(dfindex):\n",
    "    listA = dfindex.tolist()\n",
    "    listInv = []\n",
    "    for x in listA:\n",
    "        a = get_display(arabic_reshaper.reshape(x))\n",
    "        listInv.append(a)\n",
    "    return listInv    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiz=plt.figure(figsize=(15,30))\n",
    "for i in range(5):\n",
    "    df=pd.DataFrame(lda.show_topic(i), columns=['term','prob']).set_index('term')\n",
    "    #df=df.sort_values('prob')# uncomment to sort in ascending order\n",
    "    reshapedList = reshapeText(df.index)\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.title('topic '+str(i+1))\n",
    "    sns.barplot(x='prob', y=reshapedList, data=df, palette='Reds_d')#'Greens_d', Blues_d\n",
    "    plt.xlabel('probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
